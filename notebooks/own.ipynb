{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc8ac3a",
   "metadata": {},
   "source": [
    "## Loading functions and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a18c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "# from functions import load_data_CMU, load_data, connect_neighbors, search_and_evaluate, analysis, generate_graph_html, mean_squared_error, mean_pixelwise_joint_squared_error, percentage_correct_keypoints, sliding_window\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import librosa\n",
    "from numba import jit\n",
    "from matplotlib import patches\n",
    "import libfmp.b\n",
    "import libfmp.c3\n",
    "import libfmp.c7\n",
    "import networkx as nx\n",
    "from dtw import *\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "\n",
    "query = None\n",
    "dataset = None\n",
    "matrix = None\n",
    "indices = None\n",
    "motion = None\n",
    "data_eval = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77a6d0",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "559cc555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(query_path,dataset_path):\n",
    "    mat_file = h5py.File(query_path, 'r')\n",
    "    mat_file2 = h5py.File(dataset_path, 'r')\n",
    "    Q = mat_file['queryDataset']['pos']\n",
    "    D = mat_file2['wholeDataset']['pos']\n",
    "    Qnp = np.array(Q)\n",
    "    Dnp = np.array(D)\n",
    "    query = Qnp\n",
    "    dataset = Dnp\n",
    "    return query, dataset\n",
    "\n",
    "\n",
    "def load_data_CMU(query_path,dataset_path):\n",
    "    query = np.load(query_path)\n",
    "    dataset = np.load(dataset_path)\n",
    "\n",
    "    return query, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d013575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape:  (2756, 93)\n",
      "Dataset shape:  (1132475, 93)\n",
      "Time taken to load data:  5.2776031494140625  seconds\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "\n",
    "s_load = time.time()\n",
    "# query, dataset = load_data('queryDataset.mat', 'wholeDataset.mat')\n",
    "query, dataset = load_data_CMU('query.npy','Dataset.npy')\n",
    "e_load = time.time() - s_load\n",
    "\n",
    "query_shape = str(query.shape) if query is not None else None\n",
    "dataset_shape = str(dataset.shape) if dataset is not None else None\n",
    "print(\"Query shape: \",query_shape)\n",
    "print(\"Dataset shape: \",dataset_shape)\n",
    "\n",
    "print(\"Time taken to load data: \",e_load,' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793201e8",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc6179d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def mean_squared_error(vec1, vec2):\n",
    "    \"\"\"Compute the Mean Squared Error (MSE) between two vector arrays.\"\"\"\n",
    "    return np.mean((vec1 - vec2) ** 2)\n",
    "\n",
    "def search_and_evaluate(query, dataset, K, algorithm='auto', metric='minkowski', p=2, radius=1.0):\n",
    "    \"\"\"\n",
    "    Perform K-nearest neighbor search and evaluate using given parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - query: np.ndarray, the query matrix.\n",
    "    - dataset: np.ndarray, the dataset matrix.\n",
    "    - K: int, the number of nearest neighbors to retrieve.\n",
    "    - algorithm: str, algorithm to compute the nearest neighbors ('auto', 'ball_tree', 'kd_tree', 'brute').\n",
    "    - metric: str, the distance metric to use for the tree.\n",
    "    - p: int, power parameter for the Minkowski metric.\n",
    "    - radius: float, radius for radius neighbors search.\n",
    "\n",
    "    Returns:\n",
    "    - indices: np.ndarray, the indices of the nearest neighbors.\n",
    "    - data_eval: dict, dictionary containing evaluation metrics.\n",
    "    - mean_mse: float, mean of the Mean Squared Errors.\n",
    "    - mean_mpjse: float, mean of the Mean Per Joint Squared Errors.\n",
    "    - mean_pck: float, mean of the Percentages of Correct Keypoints.\n",
    "    - retrieval_time: float, the time taken for the retrieval.\n",
    "    \"\"\"\n",
    "    # Initialize the NearestNeighbors model\n",
    "    knn = NearestNeighbors(n_neighbors=K, algorithm=algorithm, leaf_size=30, metric=metric, p=p, radius=radius)\n",
    "    \n",
    "    # Fit the model on the dataset\n",
    "    knn.fit(dataset)\n",
    "    \n",
    "    # Perform K-nearest neighbor search for the query matrix\n",
    "    start_time = time.time()\n",
    "    distances, indices = knn.kneighbors(query, n_neighbors=K)\n",
    "    retrieval_time = time.time() - start_time\n",
    "    \n",
    "    # Initialize variables to store evaluation metrics\n",
    "    mse_list = []\n",
    "    mpjse_list = []\n",
    "    pck_list = []\n",
    "    \n",
    "    # Calculate evaluation metrics for each query pose\n",
    "    for i in range(len(query)):\n",
    "        # Get the K-nearest neighbors from the dataset\n",
    "        nearest_neighbors = dataset[indices[i]]\n",
    "        \n",
    "        # Calculate Mean Squared Error (MSE)\n",
    "        mse = mean_squared_error(query[i], nearest_neighbors)\n",
    "        mse_list.append(mse)\n",
    "        \n",
    "        # Calculate Mean Per Joint Squared Error (MPJSE)\n",
    "        mpjse = np.mean(np.linalg.norm(query[i] - nearest_neighbors, axis=1))\n",
    "        mpjse_list.append(mpjse)\n",
    "        \n",
    "        # Compute PCK (Percentage of Correct Keypoints)\n",
    "        threshold = 30  # Set a threshold for correctness\n",
    "        correct_keypoints = np.linalg.norm(query[i] - nearest_neighbors, axis=1) < threshold\n",
    "        pck = np.sum(correct_keypoints) / len(correct_keypoints)\n",
    "        pck_list.append(pck)\n",
    "        \n",
    "    # Calculate the mean of each evaluation metric\n",
    "    mean_mse = np.mean(mse_list)\n",
    "    mean_mpjse = np.mean(mpjse_list)\n",
    "    mean_pck = np.mean(pck_list)\n",
    "    \n",
    "    # Prepare evaluation data for output\n",
    "    data_eval = {\n",
    "        \"Mean Squared Error (MSE)\": mean_mse,\n",
    "        \"Mean Per Joint Squared Error (MPJSE)\": mean_mpjse,\n",
    "        \"Percentage of Correct Keypoints (PCK)\": mean_pck,\n",
    "        \"Retrieval Time\": retrieval_time\n",
    "    }\n",
    "    \n",
    "    # Print and return results\n",
    "    print(\"Evaluation Results:\")\n",
    "    for metric_name, value in data_eval.items():\n",
    "        print(f\"{metric_name}: {value}\")\n",
    "    \n",
    "    return indices, data_eval, mean_mse, mean_mpjse, mean_pck, retrieval_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb344f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using algorithm=kd_tree, metric=minkowski, radius=2.0\n",
      "Evaluation Results:\n",
      "Mean Squared Error (MSE): 4.0240654673736875\n",
      "Mean Per Joint Squared Error (MPJSE): 18.48859606773928\n",
      "Percentage of Correct Keypoints (PCK): 0.9729907474600871\n",
      "Retrieval Time: 49.08016061782837\n",
      "(2756, 128, 93)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "K = 128  # Number of nearest neighbors to retrieve\n",
    "\n",
    "# Perform search and evaluation with different algorithms and metrics\n",
    "algorithms = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "metrics = ['minkowski', 'euclidean', 'manhattan','l1','l2','cosine']\n",
    "radius_values = [1.0, 2.0, 5.0]\n",
    "\n",
    "algorithm = algorithms[2]\n",
    "metric = metrics[0]\n",
    "radius = radius_values[1]\n",
    "\n",
    "print(f\"\\nUsing algorithm={algorithm}, metric={metric}, radius={radius}\")\n",
    "indices, data_eval, mean_mse, mean_mpjse, mean_pck, retrieval_time = search_and_evaluate(query, dataset, K, algorithm=algorithm, metric=metric, p=2, radius=radius)\n",
    "\n",
    "# Retrieve the top 128 nearest neighbors' data from the dataset\n",
    "top_neighbors_data = dataset[indices[:, :K]]\n",
    "\n",
    "print(top_neighbors_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15f010",
   "metadata": {},
   "source": [
    "## K-Means Clustering to Reduce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "021d937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "\n",
    "# Set the environment variable to avoid memory leak on Windows with MKL\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# Parameters\n",
    "kmeans_nn = 64  # Desired number of nearest neighbors after clustering\n",
    "\n",
    "# Function to calculate reconstruction error\n",
    "def calculate_error(original, clustered):\n",
    "    return np.linalg.norm(original - clustered)\n",
    "\n",
    "# Step 1: Perform K-means clustering on each row's nearest neighbors and select nearest points\n",
    "new_data = []\n",
    "\n",
    "K_M_C_time = time.time()  # noting time for clustering process\n",
    "\n",
    "for i in range(top_neighbors_data.shape[0]):  # Iterate over each row\n",
    "    # Extract the nearest neighbors for the current row\n",
    "    nearest_neighbors_data = top_neighbors_data[i]\n",
    "\n",
    "    # Ensure kmeans_nn is less than or equal to the number of nearest neighbors\n",
    "    if kmeans_nn > nearest_neighbors_data.shape[0]:\n",
    "        kmeans_nn = nearest_neighbors_data.shape[0]\n",
    "\n",
    "    # Perform K-means clustering on the nearest neighbors\n",
    "    kmeans_model_nn = KMeans(n_clusters=kmeans_nn, random_state=0, n_init=10)\n",
    "    kmeans_labels_nn = kmeans_model_nn.fit_predict(nearest_neighbors_data)\n",
    "\n",
    "    # Calculate the cluster centers (new nearest neighbors)\n",
    "    centroids = kmeans_model_nn.cluster_centers_\n",
    "\n",
    "    # Find the nearest points in the original data to the centroids\n",
    "    nearest_points = []\n",
    "    for centroid in centroids:\n",
    "        distances = distance.cdist([centroid], nearest_neighbors_data, 'euclidean')\n",
    "        nearest_point_index = np.argmin(distances)\n",
    "        nearest_points.append(nearest_neighbors_data[nearest_point_index])\n",
    "\n",
    "    new_data.append(nearest_points)\n",
    "\n",
    "# Convert list to numpy array\n",
    "new_data = np.array(new_data)\n",
    "\n",
    "K_M_C_time_e = time.time() - K_M_C_time  # noting time for clustering process\n",
    "\n",
    "print(\"New data shape:\", new_data.shape)\n",
    "print(\"Total clustering time: \",K_M_C_time_e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e76f61",
   "metadata": {},
   "source": [
    "## Graph Construction to extract global alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6901982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_shortestP(matrix):\n",
    "    # Create graph and connect neighbors\n",
    "    graph = connect_neighbors(matrix)\n",
    "    \n",
    "    total_costs = []\n",
    "    S_neighbors = []\n",
    "    ASP = []\n",
    "    \n",
    "    start_t = time.time()\n",
    "    print(\"Shortest path search started now..\")\n",
    "    \n",
    "    for successor in graph.successors('S'):\n",
    "        S_neighbors.append(successor)\n",
    "    \n",
    "        # Find all shortest paths from 'S' to 'E'\n",
    "        all_shortest_paths = list(nx.all_shortest_paths(graph, source=successor, target='E', weight='weight'))\n",
    "        ASP.extend(all_shortest_paths)  # Extend instead of append to flatten the list\n",
    "    \n",
    "        # Compute the total cost of each path\n",
    "        for path in all_shortest_paths:\n",
    "            total_cost = sum(graph[u][v]['weight'] for u, v in zip(path[:-1], path[1:]))\n",
    "            total_costs.append(total_cost)\n",
    "    \n",
    "    \n",
    "    # Ensure the lengths of total_costs and ASP are the same\n",
    "    assert len(total_costs) == len(ASP), \"Mismatch between total costs and paths\"\n",
    "    \n",
    "    # Find the index of the shortest path with the lowest cost\n",
    "    min_cost_index = total_costs.index(min(total_costs))\n",
    "    \n",
    "    return ASP, total_costs, min_cost_index, graph\n",
    "\n",
    "\n",
    "\n",
    "def connect_neighbors(matrix):\n",
    "    \"\"\"Connect neighboring nodes in a grid-like manner.\"\"\"\n",
    "    rows, cols, h = matrix.shape\n",
    "    \n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Dictionary for node indexing\n",
    "    node_indexes = {}\n",
    "    \n",
    "    # Add nodes and populate node indexing dictionary\n",
    "    node_count = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            node = (i, j)\n",
    "            G.add_node(node)\n",
    "            node_indexes[node] = node_count\n",
    "            node_count += 1\n",
    "    \n",
    "    # Connect nodes according to grid-like logic\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            current_node = (i, j)\n",
    "            current_node_index = node_indexes[current_node]\n",
    "            \n",
    "            # Connect to the right neighbor\n",
    "            if j < cols - 1:\n",
    "                neighbor_node = (i, j + 1)\n",
    "                neighbor_node_index = node_indexes[neighbor_node]\n",
    "                edge_weight = np.mean(np.square(matrix[i, j] - matrix[i, j + 1]))  # Example of edge weight calculation\n",
    "                G.add_edge(current_node, neighbor_node, weight=edge_weight)\n",
    "            \n",
    "            # Connect to the bottom neighbor\n",
    "            if i < rows - 1:\n",
    "                neighbor_node = (i + 1, j)\n",
    "                neighbor_node_index = node_indexes[neighbor_node]\n",
    "                edge_weight = np.mean(np.square(matrix[i, j] - matrix[i + 1, j]))  # Example of edge weight calculation\n",
    "                G.add_edge(current_node, neighbor_node, weight=edge_weight)\n",
    "            \n",
    "            # Connect diagonally to the bottom-right neighbor\n",
    "            if i < rows - 1 and j < cols - 1:\n",
    "                neighbor_node = (i + 1, j + 1)\n",
    "                neighbor_node_index = node_indexes[neighbor_node]\n",
    "                edge_weight = np.mean(np.square(matrix[i, j] - matrix[i + 1, j + 1]))  # Example of edge weight calculation\n",
    "                G.add_edge(current_node, neighbor_node, weight=edge_weight)\n",
    "            \n",
    "            # Connect diagonally to the bottom-left neighbor\n",
    "            if i < rows - 1 and j > 0:\n",
    "                neighbor_node = (i + 1, j - 1)\n",
    "                neighbor_node_index = node_indexes[neighbor_node]\n",
    "                edge_weight = np.mean(np.square(matrix[i, j] - matrix[i + 1, j - 1]))  # Example of edge weight calculation\n",
    "                G.add_edge(current_node, neighbor_node, weight=edge_weight)\n",
    "    \n",
    "    # Add starting node 'S' at positions (0, 1), (1, 0), (2, 0), ...\n",
    "    for i in range(rows):\n",
    "        start_node = (i, 0)\n",
    "        G.add_edge('S', start_node, weight=1)\n",
    "    \n",
    "    # Add end node 'E' at the last element of each row\n",
    "    for i in range(rows):\n",
    "        end_node = (i, cols - 1)\n",
    "        G.add_edge(end_node, 'E', weight=1)\n",
    "    \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2671bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from functions import  find_all_shortest_paths,load_data_CMU,load_data, connect_neighbors, search_and_evaluate, analysis, generate_graph_html, mean_squared_error,mean_pixelwise_joint_squared_error, percentage_correct_keypoints, sliding_window\n",
    "import networkx as nx\n",
    "import time\n",
    "\n",
    "def graph_construction(matrix):\n",
    "    # Generate the graph based on the options\n",
    "    # Return HTML code for the graph\n",
    "    # part 3 Lazy neighborhood graph construction\n",
    "    # Draw the graph\n",
    "    matrix = np.array(matrix)\n",
    "    plt.figure(figsize=(40, 40))\n",
    "\n",
    "    # Positioning nodes\n",
    "    pos = {'S': (-1, 0)}  # Set position for 'S' node\n",
    "    for i, example_index in enumerate(range(matrix.shape[0])):\n",
    "        for j, neighbor_index in enumerate(range(matrix.shape[1])):\n",
    "            pos[(example_index, neighbor_index)] = (j, -i)  # Position each example and its neighbors in a square-like shape\n",
    "    pos['E'] = (matrix.shape[1] + 1, 0)  # Set position for 'E' node\n",
    "\n",
    "    # Create graph and connect neighbors\n",
    "    graph = connect_neighbors(matrix)\n",
    "\n",
    "    \n",
    "    start_t = time.time()\n",
    "    # part 4 Shortest path search\n",
    "    # Find all shortest paths, display total cost of each, and highlight the one with the lowest cost\n",
    "    all_shortest_paths, total_costs, min_cost_index, graph = find_all_shortestP(matrix)\n",
    "    \n",
    "    endtime = time.time()\n",
    "    \n",
    "    print(\"Time taken to find alignements using graph:\", endtime - start_t)\n",
    "\n",
    "#     # Highlight the shortest path\n",
    "    shortest_path_nodes = all_shortest_paths[min_cost_index]\n",
    "\n",
    "    # Drawing nodes and edges\n",
    "    nx.draw_networkx_nodes(graph, pos, node_size=1000, node_color=\"skyblue\")\n",
    "    nx.draw_networkx_edges(graph, pos, edge_color=\"gray\")\n",
    "    nx.draw_networkx_labels(graph, pos, font_size=12, font_weight=\"bold\")\n",
    "    nx.draw_networkx_edge_labels(graph, pos, edge_labels={(u, v): round(d[\"weight\"], 2) for u, v, d in graph.edges(data=True)})\n",
    "\n",
    "#     Drawing the shortest path nodes with a different color\n",
    "    nx.draw_networkx_nodes(graph, pos, nodelist=shortest_path_nodes, node_color=\"red\", node_size=1000)\n",
    "\n",
    "    plt.title(\"Interconnected Graph with Shortest Path Highlighted\")\n",
    "    plt.axis(\"off\")  # Turn off axis\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "#     Save the graph as an image\n",
    "#     plt.savefig(\"./GGG.png\")\n",
    "#     plt.close()  # Close the plot to avoid displaying it\n",
    "\n",
    "    # part 5 DTW analysis\n",
    "    retireved_motion = shortest_path_nodes[:-1]\n",
    "\n",
    "#     print(shortest_path_nodes)\n",
    "\n",
    "    motion = []\n",
    "    for x, y in retireved_motion:\n",
    "        print(x, y)\n",
    "        motion.append(matrix[x][y])\n",
    "\n",
    "    motion = np.array(motion)\n",
    "\n",
    "    return endtime - start_t,'<img src=\"./graph.png\" alt=\"Graph\">' , shortest_path_nodes, total_costs[min_cost_index], motion \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080ab3e",
   "metadata": {},
   "source": [
    "## Graph Search using djistra shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c66ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4\n",
    "# GGH(mat2)\n",
    "t2, graph_html, shortest_path_nodes, minimum_cost, motion2 = graph_construction(new_data[:10,:10]) #Pose Quest Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time taken using PoseQuest Method: \", t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae5460",
   "metadata": {},
   "source": [
    "## Retrieved Motion Comparison Using Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511efa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose quest method motion retrieval\n",
    "\n",
    "\n",
    "def mean_squared_error(vec1, vec2):\n",
    "    \"\"\"Compute the Mean Squared Error (MSE) between two vector arrays.\"\"\"\n",
    "    return np.mean((vec1 - vec2) ** 2)\n",
    "\n",
    "def mean_pixelwise_joint_squared_error(vec1, vec2):\n",
    "    \"\"\"Compute the Mean Pixel-wise Joint Squared Error (MPJSE) between two vector arrays.\"\"\"\n",
    "    return np.mean(np.mean((vec1 - vec2) ** 2, axis=1))\n",
    "\n",
    "def percentage_correct_keypoints(vec1, vec2, threshold):\n",
    "    \"\"\"Compute the Percentage of Correct Keypoints (PCK) between two vector arrays.\"\"\"\n",
    "    distances = np.linalg.norm(vec1 - vec2, axis=1)\n",
    "    return np.mean(distances < threshold) * 100\n",
    "\n",
    "vec_array1 = motion2\n",
    "vec_array2 = query[:len(motion2)]\n",
    "mse = mean_squared_error(vec_array1, vec_array2)\n",
    "mpjse = mean_pixelwise_joint_squared_error(vec_array1, vec_array2)\n",
    "threshold = 10  # Set threshold for PCK\n",
    "pck = percentage_correct_keypoints(vec_array1, vec_array2, threshold)\n",
    "\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Pixel-wise Joint Squared Error (MPJSE):\", mpjse)\n",
    "print(\"Percentage of Correct Keypoints (PCK):\", pck)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"extracted_motion_own.npy\",motion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0db38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
